#!/usr/bin/env python3
"""
Optimized Meta Data Upload to Supabase
=====================================
Clears meta table and uploads CSV with maximum speed while maintaining 100% data quality
Processing: meta_combined_oct24_to_oct25_full.csv (34,547 records)
"""

import os
import sys
import time
import requests
import json
import pandas as pd
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from datetime import datetime
import hashlib

# Supabase configuration
SUPABASE_URL = "https://fvyghgvshobufpgaclbs.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZ2eWdoZ3ZzaG9idWZwZ2FjbGJzIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTcxNDI1MjgsImV4cCI6MjA3MjcxODUyOH0.RR_PavED-XHko85FsuLVWfWapVIJZ3l3vRPq4lszmfM"

# Optimized batch configuration
BATCH_SIZE = 1000  # Larger batches for speed
MAX_WORKERS = 8    # Increased parallelism
RETRY_ATTEMPTS = 3

# Progress tracking
progress_lock = threading.Lock()
uploaded_records = 0
total_records = 0
failed_batches = []

def clear_meta_table():
    """Clear existing meta table data"""
    print("üßπ Clearing existing meta table data...")
    
    headers = {
        'apikey': SUPABASE_KEY,
        'Authorization': f'Bearer {SUPABASE_KEY}',
        'Content-Type': 'application/json'
    }
    
    # Delete all records from meta_raw_data table
    try:
        response = requests.delete(
            f"{SUPABASE_URL}/rest/v1/meta_raw_data",
            headers=headers,
            timeout=30
        )
        
        if response.status_code in [200, 204]:
            print("‚úÖ Meta table cleared successfully")
            return True
        else:
            print(f"‚ö†Ô∏è Clear operation returned status {response.status_code}: {response.text}")
            return True  # Continue anyway as table might be empty
            
    except Exception as e:
        print(f"‚ùå Error clearing meta table: {str(e)}")
        return False

def clean_and_validate_data(df):
    """Clean and validate data with 100% quality assurance"""
    print("üßΩ Cleaning and validating data...")
    
    original_count = len(df)
    print(f"üìä Original records: {original_count}")
    
    # Column mapping to match database schema exactly
    column_mapping = {
        'Campaign name': 'campaign_name',
        'Ad set name': 'ad_set_name', 
        'Ad name': 'ad_name',
        'Day': 'reporting_starts',  # Map Day to reporting_starts
        'Amount spent (INR)': 'amount_spent_inr',
        'CPM (cost per 1,000 impressions)': 'cpm_cost_per_1000_impressions',
        'CTR (link click-through rate)': 'ctr_link_click_through_rate',
        'Ad set delivery': 'ad_set_delivery',
        'Reporting starts': 'reporting_starts_original',
        'Reporting ends': 'reporting_ends_original'
    }
    
    # Rename columns
    df = df.rename(columns=column_mapping)
    
    # Data type conversions and cleaning
    try:
        # Convert numeric columns
        numeric_columns = ['amount_spent_inr', 'cpm', 'ctr']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # Convert date columns  
        date_columns = ['day', 'reporting_starts', 'reporting_ends']
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce').dt.strftime('%Y-%m-%d')
        
        # Handle string columns - remove extra whitespace
        string_columns = ['campaign_name', 'ad_set_name', 'ad_name', 'ad_set_delivery']
        for col in string_columns:
            if col in df.columns:
                df[col] = df[col].astype(str).str.strip()
        
        # Replace NaN values appropriately
        df = df.fillna({
            'campaign_name': 'Unknown',
            'ad_set_name': 'Unknown', 
            'ad_name': 'Unknown',
            'ad_set_delivery': 'unknown',
            'amount_spent_inr': 0.0,
            'cpm': 0.0,
            'ctr': 0.0
        })
        
        # Remove any completely empty rows
        df = df.dropna(how='all')
        
        final_count = len(df)
        print(f"‚úÖ Cleaned records: {final_count}")
        print(f"üìâ Records filtered: {original_count - final_count}")
        
        return df
        
    except Exception as e:
        print(f"‚ùå Error during data cleaning: {str(e)}")
        return df

def upload_batch(batch_data, batch_num, total_batches, retry_count=0):
    """Upload a single batch with retry logic"""
    global uploaded_records
    
    headers = {
        'apikey': SUPABASE_KEY,
        'Authorization': f'Bearer {SUPABASE_KEY}',
        'Content-Type': 'application/json',
        'Prefer': 'return=minimal'  # Optimize response size
    }
    
    try:
        # Convert batch to list of dicts
        batch_records = batch_data.to_dict('records')
        
        # Upload via upsert for better performance
        response = requests.post(
            f"{SUPABASE_URL}/rest/v1/meta_raw_data",
            headers=headers,
            json=batch_records,
            timeout=60
        )
        
        if response.status_code in [200, 201]:
            with progress_lock:
                uploaded_records += len(batch_records)
                progress = (uploaded_records / total_records) * 100
                print(f"‚úÖ Batch {batch_num:3d}/{total_batches} uploaded - {len(batch_records)} records - Progress: {progress:.1f}%")
            return True, None
            
        else:
            error_msg = f"HTTP {response.status_code}: {response.text}"
            if retry_count < RETRY_ATTEMPTS:
                print(f"üîÑ Retrying batch {batch_num} (attempt {retry_count + 1}/{RETRY_ATTEMPTS})")
                time.sleep(2 ** retry_count)  # Exponential backoff
                return upload_batch(batch_data, batch_num, total_batches, retry_count + 1)
            else:
                return False, error_msg
                
    except Exception as e:
        error_msg = f"Exception: {str(e)}"
        if retry_count < RETRY_ATTEMPTS:
            print(f"üîÑ Retrying batch {batch_num} due to exception (attempt {retry_count + 1}/{RETRY_ATTEMPTS})")
            time.sleep(2 ** retry_count)
            return upload_batch(batch_data, batch_num, total_batches, retry_count + 1)
        else:
            return False, error_msg

def upload_meta_data():
    """Main upload function with optimized parallel processing"""
    global total_records, uploaded_records, failed_batches
    
    csv_file_path = "/Users/shivangpatel/Downloads/yearly files/meta_combined_oct24_to_oct25_full.csv"
    
    if not os.path.exists(csv_file_path):
        print(f"‚ùå CSV file not found: {csv_file_path}")
        return False
    
    print(f"üìÅ Loading data from: {csv_file_path}")
    
    # Load CSV with optimized settings
    try:
        df = pd.read_csv(csv_file_path, encoding='utf-8-sig')
        total_records = len(df)
        print(f"üìä Loaded {total_records} records from CSV")
        
    except Exception as e:
        print(f"‚ùå Error loading CSV: {str(e)}")
        return False
    
    # Clean and validate data
    df = clean_and_validate_data(df)
    total_records = len(df)
    
    # Clear existing data
    if not clear_meta_table():
        print("‚ùå Failed to clear meta table")
        return False
    
    # Create batches
    batches = []
    for i in range(0, len(df), BATCH_SIZE):
        batch = df.iloc[i:i + BATCH_SIZE].copy()
        batches.append((batch, i // BATCH_SIZE + 1))
    
    total_batches = len(batches)
    print(f"‚ö° Processing {total_batches} batches of ~{BATCH_SIZE} records each")
    print(f"üöÄ Using {MAX_WORKERS} parallel workers for maximum speed")
    
    start_time = time.time()
    uploaded_records = 0
    failed_batches = []
    
    # Process batches in parallel
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = []
        
        for batch_data, batch_num in batches:
            future = executor.submit(upload_batch, batch_data, batch_num, total_batches)
            futures.append((future, batch_num, batch_data))
        
        # Collect results
        for future, batch_num, batch_data in futures:
            try:
                success, error = future.result()
                if not success:
                    failed_batches.append((batch_num, error))
                    print(f"‚ùå Batch {batch_num} failed: {error}")
            except Exception as e:
                failed_batches.append((batch_num, str(e)))
                print(f"‚ùå Batch {batch_num} exception: {str(e)}")
    
    end_time = time.time()
    duration = end_time - start_time
    
    # Final results
    print(f"\nüéâ UPLOAD COMPLETE!")
    print(f"‚úÖ Successfully uploaded: {uploaded_records:,} records")
    print(f"‚ùå Failed batches: {len(failed_batches)}")
    print(f"‚è±Ô∏è  Total time: {duration:.1f} seconds")
    print(f"üöÄ Upload rate: {uploaded_records/duration:.0f} records/second")
    print(f"üìä Data quality: {(uploaded_records/total_records)*100:.1f}%")
    
    if len(failed_batches) == 0:
        print(f"üéØ Perfect upload! All {uploaded_records:,} Meta records uploaded successfully")
        return True
    else:
        print(f"‚ö†Ô∏è  {len(failed_batches)} batches failed. Check errors above.")
        return False

if __name__ == "__main__":
    print("üöÄ Meta Data Upload Starting...")
    print("=" * 60)
    print("üìà Optimized for maximum speed and 100% data quality")
    print()
    
    success = upload_meta_data()
    
    if success:
        print("\n‚úÖ Meta data upload completed successfully!")
        print("üéâ All data uploaded with 100% quality maintained")
        sys.exit(0)
    else:
        print("\n‚ùå Meta data upload completed with errors!")
        sys.exit(1)